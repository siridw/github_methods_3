---
title: "Practical_exercise_1, Methods 3, 2021, autumn semester"
author: "Sirid Wihlborg"
date: "20/09/21"
output:
  html_document: default
  pdf_document: default
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse)
```

# 3) Brushing up on the General Linear Model 

## Exercise 1
The general linear model: $Y = X \beta + \epsilon$:  
Do a linear regression, expressing fuel usage as a function of weight using the function __lm__  

```{r}
data(mtcars)
model <- lm(mpg ~ wt, data = mtcars)
```
__1. extract $\hat{\beta}$, $Y$, $\hat{Y}$, $X$ and $\epsilon$ from __model____

__i. create a plot that illustrates $Y$ and $\hat{Y}$__
    
```{r}
# Extracting values from model
b_hat = model$coefficients
y = mtcars$mpg
X = cbind(1,mtcars$wt) 
y_hat = X%*%b_hat
error = sigma(model)

# Plotting y and y_hat 
df <- tibble(x = mtcars$wt, y = y, y_hat = y_hat) # Creating a dataframe with relevant variables

ggplot(data = df)+
  geom_smooth(aes(x, y, color = "data")) + # A line representing actual data-points
  geom_smooth(aes(x, y_hat, color = "model")) + # The linear model 
  labs(x = "Weight", y = "Fuel usage", title = "Plot 1.1: Fuel Usage as a function of Weight (Linear) 
       Plot illustrating Y (red) and Y_hat (blue)")
```


__1.1 Conclusion:__ 

$\hat{\beta}$ = (37.29, -5.34)

$Y$ = mtcars$mpg = (All actual values from our dependent variable)

$X$ = cbind(1,mtcars$wt) = (Designmatrix)

$\hat{Y}$ = $X$ * $\hat{\beta}$ = (Estimated y-values)

$\epsilon$ = 3.05 = (The error term)



__2. estimate $\beta$ for a quadratic model ($y = {\beta}_{2} x^2 + {\beta}_{1} x + {\beta}_{0}$) using ordinary least squares _without_ using __lm____; $\hat{\beta} = {({X}^{T} X)}^{-1} {X}^{T} Y$

```{r}
# Squaring x-values and adding these values to my dataframe 
df$x2 <- df$x^2 
X2 <- cbind(1, x = df$x, x2 = df$x2) # creating a new design matrix for the quadratic model

b_hat2<- solve(t(X2)%*%X2)%*%t(X2)%*%y # extracting b_hat 
```


__1.2 Conclusion:__ $\hat{\beta}$ = (49.93, -13.38, 1.17)


__3. compare your acquired $\hat{\beta}$ with the output of the corresponding quadratic model created using __lm____ 

__i. create a plot that illustrates $Y$ and $\hat{Y}$__
```{r}
q_model <- lm(y ~ x + x2, data = df) # making a quadratic model using lm
tibble(b_hat2, q_model$coefficients) # comparing coefficients extracted by hand with from lm-model


# Plotting q_model and model Y and Y-hat
y_hat2 <- X2%*%b_hat2 # Finding Y_hat for the quadratic model
df$y_hat2 <- y_hat2 # Adding the values to the dataframe

# Plotting Y against Y_hat for the quadratic model
ggplot(data = df)+
  geom_smooth(aes(x, y, color = "data")) +
  geom_smooth(aes(x, y_hat2, color = "model")) +
  labs(x = "Weight", y = "Fuel usage", title = "Plot 1.3: Fuel Usage as a function of Weight (Quadratic) 
       Plot illustrating Y (red) and Y_hat (blue)")
```


__1.3 Conclusion:__
We see that the acquired $\hat{\beta}$ is identical to the output of the corresponding quadratic model. 


## Exercise 2
Compare the plotted quadratic fit to the linear fit  

__1. which seems better?__

```{r}
######  2.1 Comparing models ###### 
# Linear model 
ggplot(data = df)+
  geom_smooth(aes(x, y, color = "data")) + 
  geom_smooth(aes(x, y_hat, color = "linear_model")) +
  geom_smooth(aes(x, y_hat2, color = "q_model")) +
  labs(x = "Weight", y = "Fuel usage", title = "Plot 2.1: Fuel Usage as a function of Weight 
       Plot illustrating linear model (green) and qudratic model (blue)")

tibble("R^2 Linear model" = summary(model)$r.squared, "R^2 Quadratic model" = summary(q_model)$r.squared)
```

__Conclusion 2.1:__ The quadratic fit seems to be a better fit, both when eye-balling the plot (Plot 2.1) and comparing R^2 values. The quadratic model explains 82% of variance and the linear model explains 75%.




__2. calculate the sum of squared errors, (show the calculation based on $\epsilon$). Which fit has the lower sum?__

```{r}
# SSE for linear model 
sse <- sum((y_hat - y)^2)
# SSE for quadratic model
sse_q <- sum((y_hat2 - y)^2) 

tibble(sse, sse_q)
```


__Conclusion 2.2:__ The  SSE for the quadratic model (203.67) is lower than the SSE for the linear model (278.32). 



__3. now make a cubic fit ($y = {\beta}_{3} x^3 + {\beta}_{2} x^2 + {\beta}_{1} x + {\beta}_{0}$) and compare it to the quadratic fit__

__i. create a plot that illustrates $Y$ and $\hat{Y}$ for both the cubic and the quadratic fits (plot them in the same plot)__

__ii. compare the sum of squared errors __

__iii. what's the estimated value of the "cubic" (${\beta}_3$) parameter? Comment on this!__ 


```{r}
df$x3 <- df$x^3 # cubing the predictor and adding to df
c_model = lm(y ~ x3 + x2 + x, data = df) # Building the cubic model

X3 <- model.matrix(c_model) # creating design matrix for cubic model
y_hat3 <- X3%*%coef(c_model) # finding y_hat for the cubic model
df$y3 <- y_hat3 # adding y_hat to dataframe

# 2.3 i: Plotting cubic and quadratic models
ggplot(data = df) +
  geom_smooth(aes(x = x, y = y, colour = "Data"))+
  geom_smooth(aes(x = x, y = y_hat2, colour = "Quad model")) +
  geom_smooth(aes(x = x, y = y_hat3, colour = "Cubic model")) +
  labs(x = "Weight", y = "Fuel usage", title = "Plot 2.3: Fuel Usage as a function of Weight  
       Plot illustrating quadratic model (blue) and cubic model (red)")

# 2.3 ii: Comparing SEE
sse_c <- sum((y_hat3 - y)^2) # SSEE for cubic model is 203.67
tibble(sse, sse_q, sse_c)

# 2.3 iii: Commenting on the estimated B_hat value 
coef(c_model)[2]
```


__Conclusion 2.3__

From looking at the plot (Plot 2.3) we see almost no difference between having a quadratic model and a cubic model. 

This is also reflected in the SSE where we see that the SSE for the cubic model is almost identical to the quadratic model 

The fourth parameter in the cubic model (x3 = 0.046) is very small, meaning that the cubic element adds very little information to our model.



## Exercise 3
__Doing a logistic regression - estimating the probability that a car has automatic transmission (0) or manual transmission (1) based on its weight__

```{r}
logistic.model <- glm(am ~ wt, data = mtcars, family="binomial")
```


Probabilities live on the range $(0, 1)$ - using the so-called logit function as a "link-function" we can map these onto the range $(-\infty, \infty)$, i.e. the real numbers.  
  
What we model in this case is: $Pr(y = 1) = logit^{-1}(X \beta)$, i.e. the probability of a car having manual transmission, given its weight. $X \beta$ is called the linear predictor; compare with $Y = X \beta + \epsilon$ 
It is helpful to define the logit function and its inverse function for the following:  

```{r}
logit <- function(x){log(x / (1 - x))} 
inv.logit <- function(x){exp(x) / (1 + exp(x))}
```

__1. plot the fitted values for __logistic.model____ 
__i. what is the relation between the linear.predictors and the fitted_values of the logistic.model object?__
    
    
```{r}
# 3.1 - plot the fitted values for logistic.model
fitted_values <- fitted(logistic.model) # defining fitted values 
plot(mtcars$wt, fitted_values)
```


__Conclusion 3.1:__ 

I'm honestly not really sure how to explain the relationship, hope you can help with that in class. :) 


__2. plot the logistic function, you've estimated based on your $\hat{\beta}$, (not just the fitted values).__
__Use an _xlim_ of (0, 7)__
```{r}
df_log <- tibble(x = mtcars$wt, y = fitted_values)

ggplot(df_log, aes(x, y)) +
  geom_line(xlim = c(0, 7))
```


__i. what's the interpretation of the estimated $\hat{\beta}_0$ (the _Intercept_)__

```{r}
boot::inv.logit(logistic.model$coefficients[1])
```

When taking the inverse of the intercept value, we get .99. 

This means that if a car weighs 0 then the probability of it being manual gear is 99%

__ii. calculate the estimated probability that the Pontiac Firebird has automatic transmission, given its weight__
  
  
```{r}
# First we find the weight of the car
mtcars %>% 
  filter(row.names(mtcars) == "Pontiac Firebird") %>%
  select(wt) 

# Then we multiply the weight with slope and add the intercept
boot::inv.logit(logistic.model$coefficients[1] + 3.845*logistic.model$coefficients[2])
```
There's a 3.13 % probability that the Pontiac Firebird has automatic transmission given its weight


__3. plot quadratic fit alongside linear fit__
    i. judging visually, does adding a quadratic term make a difference?
    ii. check the details in the help of the AIC function - which of the models provide the better fit according to the AIC values and the residual deviance respectively?
    iii. in your own words, why might it be good to penalise a model like the quadratic model, we just fitted.


```{r}
# 3.3 quadratic fit
mtcars$wt2 <- mtcars$wt^2 # adding the quadratic term to dataframe
logistic.model_q <- glm(am ~ wt2 + wt, data = mtcars, family="binomial") # Making the quadratic logistic model

plot(mtcars$wt, logistic.model$coefficients[1] + logistic.model$coefficients[2]*mtcars$wt)
plot(mtcars$wt, logistic.model_q$coefficients[1] + logistic.model_q$coefficients[2]*mtcars$wt2)



# Comparing the logistic model to the quadratic logistic model
AIC(logistic.model, logistic.model_q)
  
```

__Conclusion 3.3:__ 




    
# Next time
We are going to looking at extending our models with so called random effects. We need to install the package "lme4" for this. Run the code below or install it from your package manager (Linux)  
```{r, eval=FALSE}
install.packages("lme4")
```
We can fit a model like this:

```{r}
library(lme4)
mixed.model <- lmer(mpg ~ wt + (1 | cyl), data=mtcars)
```

They result in plots like these:
```{r}
par(font.lab=2, font.axis=2, cex=1.2)
plot(mtcars$wt, fitted.values(mixed.model),
     main='Linear regression with group intercepts (n cylinders)',
    xlab='Weight (lb/1000)', ylab='Miles/(US) gallon',
    pch=3)
```

and this
```{r}
mixed.model <- lmer(mpg ~ wt + (wt | cyl), data=mtcars)
plot(mtcars$wt, fitted.values(mixed.model),
     main='Linear regression with group intercepts and group slopes (n cylinders)',
    xlab='Weight (lb/1000)', ylab='Miles/(US) gallon',
    pch=3)
``` 

but also new warnings like:  

Warning:
In checkConv(attr(opt, "derivs"), opt\$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0121962 (tol = 0.002, component 1)
